{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eee242-73d7-4678-a9d0-c2aaac242b5a",
   "metadata": {},
   "source": [
    "# Newtons metode\n",
    "\n",
    "En ulempe med fikspunktiterasjon er at når den konvergerer, vil det typisk skje nokså langsomt.\n",
    "Dette kan forbedres dersom vi har mer informasjon om funksjonen $f$ vi vil finne roten til; i fikspunktiterasjonen bruker vi kun $f$ selv til å finne en funksjon $g$ å iterere med, $x_{n+1} = g(x_n)$.\n",
    "\n",
    "Dersom vi kjenner den deriverte $f'$, vet vi mer om hvordan $f$ oppfører seg, og dette kan vi bruke til å finne en mer effektiv iterativ metode.\n",
    "Idéen er som følger: gitt en gjetning $x_n$ som ikke er roten selv, $f(x_n) \\neq 0$, kan vi finne tangenten til funksjonen $f$ i dette punktet, $y = y(x)$ gitt ved\n",
    "\n",
    "$$ y = f(x_n) + f'(x_n)(x-x_n). \\tag{2.6} $$\n",
    "\n",
    "Dersom denne tangenten ikke er horisontal, det vil si at vi ikke har $y(x) \\equiv f(x_n) \\iff f'(x_n)=0$, så vil tangenten skjære $x$-aksen i ett punkt hvor $y=0$.\n",
    "Vi antar derfor $f'(x_n) \\neq 0$ slik at dette er tilfelle.\n",
    "Dersom $x_n$ er nær nok roten $r$ er det rimelig at dette skjæringspunktet er en bedre tilnærming av $r$ enn $x_n$, så vi kan definere dette som vårt neste iterat: vi setter $y=0$ i ligning (2.6), løser for x og finner\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. \\tag{2.7} $$\n",
    "\n",
    "Dette er Newtons metode, og vi kan merke oss at vi kan se på denne som en fikspunktmetode med funksjonen $g$ gitt av\n",
    "\n",
    "$$g(x) = x - \\frac{f(x)}{f'(x)},$$\n",
    "\n",
    "som tydelig oppfyller $r = g(r)$ dersom $f(r) = 0$ og $f'(r) \\neq 0$.\n",
    "\n",
    "Dersom vi antar at $f$ er to ganger kontinuerlig deriverbar ser vi også at\n",
    "\n",
    "$$ g'(x) = \\frac{f''(x) f(x)}{(f'(x))^2}, $$\n",
    "\n",
    "som viser at dersom $r$ er en rot for $f$, så er Newtons metode lokalt konvergent mot $r$ dersom $f'(r)\\neq 0$, siden $g'(r) = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a6c841-b239-459f-b176-e0d7e5b4418f",
   "metadata": {},
   "source": [
    ">#### Eksempel:\n",
    "La oss se hvordan Newtons metode sammenligner seg med fikspunktiterasjonen (med valget av $g$ som fungerte) i et tidligere eksempel, nemlig for $f(x) = x^3+x-1$ på intervallet $[0,1]$.\n",
    "Her ser vi at $f'(x) = 1 + 3x^2 \\ge 1$, så det er ingen fare for å dele på null.\n",
    "<img src=\"2_newton.png\" style=\"width: 100%\">\n",
    "Ovenfor har vi laget fikspunktplott for $g(x) = (1-x)^{1/3}$ og $g$ tilsvarende Newtons metode, i tillegg til et plott av $f$ og tangentlinjene tilhørende Newtons metode.\n",
    "Legg merke til hvor mye raskere Newtons metode tilnærmer roten $r$, allerede ved andre iterasjon er man typisk kommet veldig nær, uavhengig av startgjetning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf98c0-ea4a-45a7-9827-cdf6319ee89c",
   "metadata": {},
   "source": [
    "Grunnen til at Newtons metode er raskere er at for $f'(r)\\neq0$ vil feilen avta kvadratisk og ikke lineært som er typisk for fikspunktiterasjoner.\n",
    "\n",
    "> **Definisjon 1**: La $e_i$ være feilen etter $i$ steg med en iterativ metode. Dersom\n",
    "$$ \\lim\\limits_{i\\to\\infty} \\frac{e_{i+1}}{e_i} = S, \\quad \\text{hvor} \\quad 0 < S < 1, \\tag{3} $$\n",
    "sier vi at metoden er *lineært konvergent* med rate $S$.\n",
    "\n",
    "Dette betyr at for store nok verdier av $i$ vil neste feil $e_{i+1}$ være omtrent lik $S e_i$.\n",
    "\n",
    "> **Definisjon 2**: La $e_i$ være feilen etter $i$ steg med en iterativ metode. Vi sier at iterasjonen *konvergerer kvadratisk* dersom\n",
    "$$ \\lim\\limits_{i\\to\\infty} \\frac{e_{i+1}}{e_i^2} = M < \\infty.  $$\n",
    "\n",
    "Uttrykket ovenfor forteller oss at for store nok verdier av $i$ vil neste feil $e_{i+1}$ være omtrent lik $M e_i^2$.\n",
    "\n",
    "Feilen $e_i$ er gitt ved avviket $e_i = |x_i-r|$, hvor $r$ er roten vi er ute etter.\n",
    "Siden $r$ typisk er ukjent vil det være vanskelig å plotte denne feilen.\n",
    "Et godt alternativ kan da være å se på avviket mellom forrige og neste iterat, $|x_{i+1}-x_i|$.\n",
    "For en fikspunktmetode vil dette tilsvare $|g(x_i)-x_i|$, som også er et mål på hvor nær vi er fikspunktet $r = g(r)$.\n",
    "\n",
    "La oss sammenligne hvordan feilen avtar når vi utfører $N$ iterasjoner for de to metodene i forrige eksempel.\n",
    "<img src=\"2_fikspunktrate.png\" style=\"width: 100%\">\n",
    "Som en tilnærming til raten $S$ har vi tatt gjennomsnittsverdien av $e_{i+1}/e_{i}$ for $i = 0,...,N-1$ og i stedet brukt denne verdien.\n",
    "I plottet til venstre har vi plottet $|x_{k+1}-x_k|$ for hver iterasjon, og vi ser at disse følger kurven $S^k |x_1-x_0|$ veldig godt.\n",
    "\n",
    "Merk at dersom man har en feil $e_k$ som oppfyller den geometriske sammenhengen $e_{k} = S^k e_0$, kan vi ta logaritmen av begge sider og i stedet få $\\log_{10}(e_k) = k \\log_{10}(S) + \\log_{10}(e_0)$.\n",
    "Altså er logaritmen av feilen en lineær funksjon av $k$.\n",
    "Siden $S^k$ raskt blir veldig liten for $0 < S < 1$ og store verdier av $k$, er det ofte nyttig å skalere $y$-aksen i et feilplott logaritmisk istedenfor lineært:\n",
    "det vil si, når vi går én enhet langs $x$-aksen går vi fra $x=k$ til $x=k+1$, men langs $y$-aksen går vi i stedet fra $y = 10^{k}$ til $y = 10^{k+1}$.\n",
    "Dette er hva vi har gjort i plottet til høyre, hvor vi har brukt kommandoen $\\texttt{pyplot.semilogy}()$ istedenfor $\\texttt{pyplot.plot}()$, og vi ser at feilene nå ligger mer eller mindre langs en rett linje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ccece-7608-49dd-adff-99de823dd665",
   "metadata": {},
   "source": [
    "For fikspunktiterasjonen ovenfor avtar ikke feilene så raskt at de blir vanskelige å skille frahverandre i normalt plott, men for Newtons metode vil vi se hvorfor det logaritmiske plottet er nyttig.\n",
    "<img src=\"2_newtonrate.png\" style=\"width: 100%\">\n",
    "Her har vi plottet feilene vi fikk med Newtons metode mot funksjoner med kvadratisk og lineært avtakende potenser, $10^{-k^2}|x_1-x_0|$ og $10^{-k}|x_1-x_0|$.\n",
    "Plottet til høyre er lineært skalert langs både $x$- og $y$-aksen, og vi ser at både feilen og funksjonene avtar raskt med $k$; det er også vakselig å skille funksjonene fra hverandre med det blotte øye for $k > 2$.\n",
    "\n",
    "I plottet til høyre, hvor $y$-aksen er logaritmisk skalert, ser vi lettere forskjell på funksjonene. I tillegg ser vi at feilen avtar raskere enn lineært, og dermed stemmer bedre overens med den kvadratiske potensen; vi kunne fått enda bedre overenstemmelse med kurven om vi hadde justert grunntallet $10^{-1}$ til raten vi får i akkurat dette tilfellet, slik vi gjorde med raten $S$ for fikspunktmetoden ovenfor.\n",
    "\n",
    "Her ser vi også at noen av punktene som dukker opp i det venstre plottet ikke synes i det høyre, nemlig for $k > 5$.\n",
    "Dette kan skje dersom feilene blir null, siden det ville tilsvart en uendelig negativ potens, $$ 0 = \\lim_\\limits{p\\to \\infty}10^{-p}, $$\n",
    "som det ikke gir mening å plotte.\n",
    "\n",
    "> **Merknad**: Noen ganger har Newtons metode kun lineær konvergens, dette kan skje dersom $f'(r) = 0$ i roten $r$.\n",
    "\n",
    "**Kilder**: Sauer kap. 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066bc9d-6153-4562-95a3-6d36c14f4885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98eee242-73d7-4678-a9d0-c2aaac242b5a",
   "metadata": {},
   "source": [
    "# Newtons metode\n",
    "\n",
    "En ulempe med fikspunktiterasjon er at når den konvergerer, vil det typisk skje nokså langsomt.\n",
    "Dette kan forbedres dersom vi har mer informasjon om funksjonen $f$ vi vil finne roten til; i fikspunktiterasjonen bruker vi kun $f$ selv til å finne en funksjon $g$ å iterere med, $x_{n+1} = g(x_n)$.\n",
    "\n",
    "Dersom vi kjenner den deriverte $f'$, vet vi mer om hvordan $f$ oppfører seg, og dette kan vi bruke til å finne en mer effektiv iterativ metode.\n",
    "Idéen er som følger: gitt en gjetning $x_n$ som ikke er roten selv, $f(x_n) \\neq 0$, kan vi finne tangenten til funksjonen $f$ i dette punktet, $y = y(x)$ gitt ved\n",
    "\n",
    "$$ y = f(x_n) + f'(x_n)(x-x_n). \\tag{2.6} $$\n",
    "\n",
    "Dersom denne tangenten ikke er horisontal, det vil si at vi ikke har $y(x) \\equiv f(x_n) \\iff f'(x_n)=0$, så vil tangenten skjære $x$-aksen i ett punkt hvor $y=0$.\n",
    "Vi antar derfor $f'(x_n) \\neq 0$ slik at dette er tilfelle, se figuren under.\n",
    "\n",
    "<img src=\"2_newton_illustration.png\" style=\"width: 30%\">\n",
    "\n",
    "I figuren over har vi $n$-te iterat $x_n$, og vi kan finne tangenten til grafen $y=f(x)$ i punktet $x = x_n$.\n",
    "Tangenten er den blå stiplede linjen, og skjærer $x$-aksen i punktet $x_{n+1}$.\n",
    "Dersom $x_n$ er nær nok roten $r$ er det rimelig at dette skjæringspunktet er en bedre tilnærming av roten $r$ enn $x_n$, så vi kan definere dette som vårt neste iterat: vi setter $y=0$ i ligning (2.6), løser for x og finner\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. \\tag{2.7} $$\n",
    "\n",
    "Dette er Newtons metode, og vi kan merke oss at vi kan se på denne som en fikspunktiterasjon, $x_{n+1} = g(x_n)$, med funksjon $g$ gitt av\n",
    "\n",
    "$$g(x) = x - \\frac{f(x)}{f'(x)},$$\n",
    "\n",
    "som tydelig oppfyller $r = g(r)$ dersom $f(r) = 0$ og $f'(r) \\neq 0$.\n",
    "\n",
    "Dersom vi antar at $f$ er to ganger kontinuerlig deriverbar ser vi også at\n",
    "\n",
    "$$ g'(x) = 1 - \\frac{f'(x)f'(x)-f(x)f''(x)}{(f'(x))^2} = \\frac{f(x) f''(x)}{(f'(x))^2}, $$\n",
    "\n",
    "som viser at dersom $r$ er en rot for $f$, så er Newtons metode lokalt konvergent mot $r$ dersom $f'(r)\\neq 0$, siden $g'(r) = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ada0-c71c-40a5-9515-cf5d4db0ade1",
   "metadata": {},
   "source": [
    ">#### Eksempel:\n",
    "I et tidligere eksempel prøvde vi å tilnærme løsningen av ligningen av $f(x) = 0$ for funksjonen $e^x-2x-1$ ved hjelp av fikspunktiterasjon. La oss nå i stedet bruke Newtons metode.\n",
    ">\n",
    ">Den deriverte er $f'(x) = e^x -2$, slik at Newton-iterasjonen er gitt av\n",
    ">\n",
    ">$$ x_{n+1} = x_n - \\frac{e^{x_n}-2x_n-1}{e^{x_n}-2}, \\qquad n = 0,1,\\dots $$\n",
    ">\n",
    ">La oss velge startgjettet $x_0 = 1.5$, da har vi et avvik for funksjonen $f$ gitt av $|f(1.5)| = e^{1.5}-4 \\approx 0.482$.\n",
    "Vi finner fra formelen over at første iterat er\n",
    ">\n",
    ">$$ x_1 = 1 - \\frac{e^{1.5}-4}{e^{1.5}-2} = 1.3059...,$$\n",
    ">\n",
    ">som gir det tilhørende avviket $|f(x_1)| \\approx 0.0792$. Dette er omtrent seks ganger mindre enn før. Andre iterat er så gitt av\n",
    ">\n",
    ">$$ x_2 = x_1 - \\frac{e^{x_1}-2x_1-1}{e^{x_1}-2} = 1.2590...$$\n",
    ">\n",
    ">med tilhørende avvik $|f(x_2)| \\approx 0.00399$, som er mye mindre enn før, faktisk nesten tjue ganger mindre. Slik kan vi fortsette til vi synes avviket er lite nok.\n",
    "\n",
    "Nedenfor er en mulig implementasjon av Newtons metode, som vi kan bruke på eksempelet ovenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d340ac1b-7d11-436e-83d2-17e472dad7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.259058728409144, 0.0039872127133326885, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(x)-2*x-1\n",
    "\n",
    "def df(x):\n",
    "    return np.exp(x)-2\n",
    "\n",
    "def newton(f,df,x0,tol,maxIt):\n",
    "    # implementasjon av Newtons metode for en funksjon f, dens deriverte df,\n",
    "    # startgjett x0, feiltoleranse tol and maksimalt antall iterasjoner maxIt.\n",
    "    # Returnerer siste iterat, feilen and antall iterasjoner\n",
    "    x = x0\n",
    "    It = 0\n",
    "    err = np.abs(f(x))\n",
    "    \n",
    "    while err > tol and It < maxIt:\n",
    "        x = x - f(x)/df(x)\n",
    "        err = np.abs(f(x))\n",
    "        It = It + 1\n",
    "    \n",
    "    return x, err, It\n",
    "\n",
    "# Vi bruker startgjett 1.5, ønsker en feil mindre enn eller lik 0.01, og maksimalt antall iterasjoner 10\n",
    "newton(f,df,1.5,1e-2,10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968dc6c8-7dad-458f-a2c3-b17b1808ad65",
   "metadata": {},
   "source": [
    "> $\\impliedby$ [Andre fallgruver og lokal konvergens](2_lignlos_fikspunkt_lokal.ipynb) | [Konvergensrater for iterative metoder](2_lignlos_rater.ipynb) $\\implies$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfd08e-6649-41a3-962d-2bdf80ed259e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
